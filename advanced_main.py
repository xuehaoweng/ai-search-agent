#!/usr/bin/env python3
"""
é«˜çº§AIæœç´¢ä»£ç† - å¢å¼ºç‰ˆ
æ”¯æŒå¤šç§æœç´¢ç±»å‹ã€æœ‰çŠ¶æ€å¯¹è¯ã€æµå¼å“åº”å’Œå¤æ‚å·¥ä½œæµ
"""

import asyncio
import os
import json
from typing import Dict, Any, Optional
from dotenv import load_dotenv

from stateful_agent import StatefulSearchAgent, ConversationManager
from streaming_workflow import StreamingWorkflowEngine, AsyncTaskQueue
from schemas import SearchConfig, SearchType, StreamChunk

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class AdvancedSearchSystem:
    """é«˜çº§æœç´¢ç³»ç»Ÿ"""
    
    def __init__(self):
        self.conversation_manager = ConversationManager()
        self.workflow_engine = StreamingWorkflowEngine(self.conversation_manager)
        self.task_queue = AsyncTaskQueue(max_concurrent_tasks=3)
        
        # é»˜è®¤æ¨¡å‹é…ç½®
        self.default_model_config = self._get_model_config()
        
        # åˆ›å»ºé»˜è®¤ä»£ç†
        self.default_agent = self.conversation_manager.create_agent(
            "default", 
            self.default_model_config
        )
    
    def _get_model_config(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹é…ç½®"""
        # æ£€æŸ¥å¯ç”¨çš„APIå¯†é’¥å¹¶é€‰æ‹©æœ€ä½³æ¨¡å‹
        zhipu_key = os.getenv('ZHIPU_API_KEY')
        if zhipu_key:
            return {
                'api_key': zhipu_key,
                'base_url': 'https://open.bigmodel.cn/api/paas/v4',
                'model_name': 'glm-4-flash'
            }
        
        deepseek_key = os.getenv('DEEPSEEK_API_KEY')
        if deepseek_key:
            return {
                'api_key': deepseek_key,
                'base_url': 'https://api.deepseek.com',
                'model_name': 'deepseek-chat'
            }
        
        openai_key = os.getenv('OPENAI_API_KEY')
        if openai_key:
            return {
                'api_key': openai_key,
                'model_name': 'gpt-3.5-turbo'
            }
        
        raise ValueError("æœªæ‰¾åˆ°å¯ç”¨çš„APIå¯†é’¥ï¼Œè¯·è®¾ç½® ZHIPU_API_KEYã€DEEPSEEK_API_KEY æˆ– OPENAI_API_KEY")
    
    async def start(self):
        """å¯åŠ¨ç³»ç»Ÿ"""
        await self.task_queue.start_workers()
        print("ğŸš€ é«˜çº§AIæœç´¢ç³»ç»Ÿå·²å¯åŠ¨")
        print(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹: {self.default_model_config.get('model_name')}")
        print(f"ğŸ”§ å¯ç”¨å·¥ä½œæµæ¨¡æ¿: {', '.join(self.workflow_engine.list_workflow_templates())}")
    
    async def stop(self):
        """åœæ­¢ç³»ç»Ÿ"""
        await self.task_queue.stop_workers()
        print("ğŸ›‘ ç³»ç»Ÿå·²åœæ­¢")
    
    async def simple_search(self, query: str, search_type: str = "general") -> Dict[str, Any]:
        """ç®€å•æœç´¢"""
        config = SearchConfig(
            search_type=SearchType(search_type),
            max_results=5,
            include_summary=True
        )
        
        result = await self.default_agent.search_tools.smart_search(query, config)
        return result.dict()
    
    async def conversational_search(self, query: str, conversation_id: Optional[str] = None) -> Dict[str, Any]:
        """å¯¹è¯å¼æœç´¢"""
        if conversation_id is None:
            conversation_id = await self.default_agent.start_conversation()
        
        result = await self.default_agent.chat(conversation_id, query)
        return result
    
    async def streaming_search(self, query: str, conversation_id: Optional[str] = None):
        """æµå¼æœç´¢"""
        if conversation_id is None:
            conversation_id = await self.default_agent.start_conversation()
        
        async for chunk in self.default_agent.chat_stream(conversation_id, query):
            yield chunk
    
    async def workflow_search(self, workflow_template: str, params: Dict[str, Any]):
        """å·¥ä½œæµæœç´¢"""
        async for chunk in self.workflow_engine.execute_workflow_stream(
            workflow_template, params, "default"
        ):
            yield chunk
    
    async def interactive_mode(self):
        """äº¤äº’æ¨¡å¼"""
        print("\nğŸ¤– æ¬¢è¿ä½¿ç”¨é«˜çº§AIæœç´¢ä»£ç†ï¼")
        print("ğŸ’¡ è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©ï¼Œè¾“å…¥ 'quit' é€€å‡º")
        print("=" * 50)
        
        conversation_id = await self.default_agent.start_conversation()
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ æ‚¨: ").strip()
                
                if not user_input:
                    continue
                
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                    break
                
                if user_input.lower() == 'help':
                    self._show_help()
                    continue
                
                if user_input.startswith('/workflow'):
                    await self._handle_workflow_command(user_input)
                    continue
                
                if user_input.startswith('/stream'):
                    query = user_input[7:].strip()
                    if query:
                        await self._handle_streaming_search(query, conversation_id)
                    else:
                        print("âŒ è¯·æä¾›æœç´¢æŸ¥è¯¢")
                    continue
                
                # æ™®é€šå¯¹è¯æœç´¢
                print("\nğŸ¤– AI: ", end="", flush=True)
                result = await self.default_agent.chat(conversation_id, user_input)
                
                if 'error' in result:
                    print(f"âŒ {result['error']}")
                else:
                    print(result['response'])
                    
                    # æ˜¾ç¤ºå»ºè®®
                    if result.get('suggestions'):
                        print(f"\nğŸ’¡ ç›¸å…³å»ºè®®: {', '.join(result['suggestions'][:3])}")
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        
        await self.default_agent.end_conversation(conversation_id)
    
    def _show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        help_text = """
ğŸ” é«˜çº§AIæœç´¢ä»£ç† - å¸®åŠ©ä¿¡æ¯

åŸºæœ¬æœç´¢:
  ç›´æ¥è¾“å…¥é—®é¢˜å³å¯è¿›è¡Œæ™ºèƒ½æœç´¢

ç‰¹æ®Šå‘½ä»¤:
  /stream <æŸ¥è¯¢>     - æµå¼æœç´¢å“åº”
  /workflow <æ¨¡æ¿>   - æ‰§è¡Œå¤æ‚å·¥ä½œæµ
  help              - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
  quit/exit         - é€€å‡ºç¨‹åº

å¯ç”¨å·¥ä½œæµæ¨¡æ¿:
  comprehensive_search    - ç»¼åˆæœç´¢åˆ†æ
  multi_source_analysis  - å¤šæºä¿¡æ¯åˆ†æ
  research_report        - ç ”ç©¶æŠ¥å‘Šç”Ÿæˆ
  competitive_analysis   - ç«äº‰åˆ†ææŠ¥å‘Š
  trend_analysis         - è¶‹åŠ¿åˆ†ææŠ¥å‘Š

ç¤ºä¾‹:
  ç”Ÿæˆå¼AIçš„æœ€æ–°è¿›å±•
  /stream äººå·¥æ™ºèƒ½å‘å±•è¶‹åŠ¿
  /workflow comprehensive_search topic=æœºå™¨å­¦ä¹ 

åŠŸèƒ½ç‰¹ç‚¹:
  âœ… æ™ºèƒ½æœç´¢ç±»å‹è¯†åˆ«ï¼ˆæ–°é—»ã€å­¦æœ¯ã€äº§å“ç­‰ï¼‰
  âœ… å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡è®°å¿†
  âœ… æµå¼å“åº”å®æ—¶åé¦ˆ
  âœ… å¤æ‚å·¥ä½œæµè‡ªåŠ¨åŒ–
  âœ… å¤šå·¥å…·é›†æˆï¼ˆæœç´¢ã€æ‘˜è¦ã€ç¿»è¯‘ç­‰ï¼‰
        """
        print(help_text)
    
    async def _handle_workflow_command(self, command: str):
        """å¤„ç†å·¥ä½œæµå‘½ä»¤"""
        parts = command.split()
        if len(parts) < 2:
            print("âŒ ç”¨æ³•: /workflow <æ¨¡æ¿å> [å‚æ•°]")
            print(f"å¯ç”¨æ¨¡æ¿: {', '.join(self.workflow_engine.list_workflow_templates())}")
            return
        
        template = parts[1]
        
        # è§£æå‚æ•°
        params = {}
        for part in parts[2:]:
            if '=' in part:
                key, value = part.split('=', 1)
                params[key] = value
        
        # å¦‚æœæ²¡æœ‰æä¾›å¿…è¦å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if template in ['comprehensive_search', 'multi_source_analysis', 'research_report'] and 'query' not in params and 'topic' not in params:
            query = input("è¯·è¾“å…¥æœç´¢ä¸»é¢˜: ").strip()
            if template == 'comprehensive_search':
                params['query'] = query
            else:
                params['topic'] = query
        
        print(f"\nğŸš€ æ‰§è¡Œå·¥ä½œæµ: {template}")
        print("=" * 30)
        
        try:
            async for chunk in self.workflow_engine.execute_workflow_stream(
                template, params, "default"
            ):
                if chunk.chunk_type == "text":
                    print(f"ğŸ“ {chunk.content}")
                elif chunk.chunk_type == "result":
                    content = chunk.content
                    if isinstance(content, dict):
                        step_name = content.get('step_name', 'æœªçŸ¥æ­¥éª¤')
                        if 'error' in content:
                            print(f"âŒ {step_name}: {content['error']}")
                        else:
                            print(f"âœ… {step_name}: å®Œæˆ")
                elif chunk.chunk_type == "complete":
                    content = chunk.content
                    if isinstance(content, dict) and 'error' not in content:
                        print(f"\nğŸ‰ å·¥ä½œæµå®Œæˆï¼")
                        print(f"â±ï¸ è€—æ—¶: {content.get('duration', 0):.2f}ç§’")
                        print(f"âœ… å®Œæˆæ­¥éª¤: {content.get('steps_completed', 0)}")
                    else:
                        print(f"\nâŒ å·¥ä½œæµå¤±è´¥: {content.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        except Exception as e:
            print(f"âŒ å·¥ä½œæµæ‰§è¡Œé”™è¯¯: {e}")
    
    async def _handle_streaming_search(self, query: str, conversation_id: str):
        """å¤„ç†æµå¼æœç´¢"""
        print(f"\nğŸ” æµå¼æœç´¢: {query}")
        print("=" * 30)
        
        try:
            async for chunk in self.default_agent.chat_stream(conversation_id, query):
                if chunk.chunk_type == "text":
                    print(f"ğŸ“ {chunk.content}")
                elif chunk.chunk_type == "result":
                    content = chunk.content
                    if isinstance(content, dict):
                        print(f"ğŸ“Š æœç´¢ç»“æœå·²è·å–")
                elif chunk.chunk_type == "summary":
                    print(f"\nğŸ¤– AIåˆ†æ:\n{chunk.content}")
                elif chunk.chunk_type == "complete":
                    content = chunk.content
                    if isinstance(content, dict) and 'suggestions' in content:
                        suggestions = content['suggestions']
                        if suggestions:
                            print(f"\nğŸ’¡ ç›¸å…³å»ºè®®: {', '.join(suggestions[:3])}")
        
        except Exception as e:
            print(f"âŒ æµå¼æœç´¢é”™è¯¯: {e}")

# å‘½ä»¤è¡Œæ¥å£
async def main():
    """ä¸»å‡½æ•°"""
    system = AdvancedSearchSystem()
    
    try:
        await system.start()
        
        # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
        import sys
        if len(sys.argv) > 1:
            command = sys.argv[1]
            
            if command == "search" and len(sys.argv) > 2:
                # ç®€å•æœç´¢æ¨¡å¼
                query = " ".join(sys.argv[2:])
                result = await system.simple_search(query)
                print(json.dumps(result, indent=2, ensure_ascii=False))
            
            elif command == "workflow" and len(sys.argv) > 3:
                # å·¥ä½œæµæ¨¡å¼
                template = sys.argv[2]
                params = {}
                for arg in sys.argv[3:]:
                    if '=' in arg:
                        key, value = arg.split('=', 1)
                        params[key] = value
                
                async for chunk in system.workflow_search(template, params):
                    if chunk.chunk_type == "complete":
                        print(json.dumps(chunk.content, indent=2, ensure_ascii=False))
            
            elif command == "interactive":
                # äº¤äº’æ¨¡å¼
                await system.interactive_mode()
            
            else:
                print("ç”¨æ³•:")
                print("  python advanced_main.py interactive           # äº¤äº’æ¨¡å¼")
                print("  python advanced_main.py search <æŸ¥è¯¢>         # ç®€å•æœç´¢")
                print("  python advanced_main.py workflow <æ¨¡æ¿> <å‚æ•°> # å·¥ä½œæµæœç´¢")
        
        else:
            # é»˜è®¤äº¤äº’æ¨¡å¼
            await system.interactive_mode()
    
    finally:
        await system.stop()

if __name__ == "__main__":
    asyncio.run(main())